{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import EvalPrediction, Trainer, TrainingArguments, DistilBertTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = self.preprocess_text(texts)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Check if we have labels\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long),\n",
    "            }\n",
    "        else:  # Return only inputs for test data\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            }\n",
    "\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        preprocessed_texts = []\n",
    "        for text in texts:\n",
    "            # Apply SpaCy pipeline on the text\n",
    "            doc = nlp(text)\n",
    "            # Lemmatize the text and join the words back into a single string\n",
    "            lemma_text = \" \".join([token.lemma_ for token in doc])\n",
    "            preprocessed_texts.append(lemma_text)\n",
    "        return preprocessed_texts\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN  Ruby AK  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1   \n",
      "\n",
      "    id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data\n",
    "train_df = pd.read_csv('../data/twitter/train.csv')\n",
    "test_df = pd.read_csv('../data/twitter/test.csv')\n",
    "print(train_df.head(),'\\n\\n', test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4    None     None             Forest fire near La Ronge Sask. Canada   \n",
      "2   5    None     None  All residents asked to 'shelter in place' are ...   \n",
      "3   6    None     None  13,000 people receive #wildfires evacuation or...   \n",
      "4   7    None  Ruby AK  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1   \n",
      "\n",
      "    id keyword location                                               text\n",
      "0   0    None     None                 Just happened a terrible car crash\n",
      "1   2    None     None  Heard about #earthquake is different cities, s...\n",
      "2   3    None     None  there is a forest fire at spot pond, geese are...\n",
      "3   9    None     None           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11    None     None      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values by filling with a placeholder value\n",
    "train_df = train_df.fillna('None')\n",
    "test_df = test_df.fillna('None')\n",
    "print(train_df.head(),'\\n\\n', test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4    None     None             Forest fire near La Ronge Sask. Canada   \n",
      "2   5    None     None  All residents asked to 'shelter in place' are ...   \n",
      "3   6    None     None  13,000 people receive #wildfires evacuation or...   \n",
      "4   7    None  Ruby AK  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0     1.0  \n",
      "1     1.0  \n",
      "2     1.0  \n",
      "3     1.0  \n",
      "4     1.0  \n"
     ]
    }
   ],
   "source": [
    "# Combine train and test data\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id location                                               text  target  \\\n",
      "0   1     None  Our Deeds are the Reason of this #earthquake M...     1.0   \n",
      "1   4     None             Forest fire near La Ronge Sask. Canada     1.0   \n",
      "2   5     None  All residents asked to 'shelter in place' are ...     1.0   \n",
      "3   6     None  13,000 people receive #wildfires evacuation or...     1.0   \n",
      "4   7  Ruby AK  Just got sent this photo from Ruby #Alaska as ...     1.0   \n",
      "\n",
      "   keyword_None  keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
      "0          True           False             False               False   \n",
      "1          True           False             False               False   \n",
      "2          True           False             False               False   \n",
      "3          True           False             False               False   \n",
      "4          True           False             False               False   \n",
      "\n",
      "   keyword_airplane%20accident  keyword_ambulance  ...  keyword_weapons  \\\n",
      "0                        False              False  ...            False   \n",
      "1                        False              False  ...            False   \n",
      "2                        False              False  ...            False   \n",
      "3                        False              False  ...            False   \n",
      "4                        False              False  ...            False   \n",
      "\n",
      "   keyword_whirlwind  keyword_wild%20fires  keyword_wildfire  \\\n",
      "0              False                 False             False   \n",
      "1              False                 False             False   \n",
      "2              False                 False             False   \n",
      "3              False                 False             False   \n",
      "4              False                 False             False   \n",
      "\n",
      "   keyword_windstorm  keyword_wounded  keyword_wounds  keyword_wreck  \\\n",
      "0              False            False           False          False   \n",
      "1              False            False           False          False   \n",
      "2              False            False           False          False   \n",
      "3              False            False           False          False   \n",
      "4              False            False           False          False   \n",
      "\n",
      "   keyword_wreckage  keyword_wrecked  \n",
      "0             False            False  \n",
      "1             False            False  \n",
      "2             False            False  \n",
      "3             False            False  \n",
      "4             False            False  \n",
      "\n",
      "[5 rows x 226 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the 'keyword' field\n",
    "combined_df = pd.get_dummies(combined_df, columns=['keyword'])\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id location                                               text  target  \\\n",
      "0   1     None  Our Deeds are the Reason of this #earthquake M...     1.0   \n",
      "1   4     None             Forest fire near La Ronge Sask. Canada     1.0   \n",
      "2   5     None  All residents asked to 'shelter in place' are ...     1.0   \n",
      "3   6     None  13,000 people receive #wildfires evacuation or...     1.0   \n",
      "4   7  Ruby AK  Just got sent this photo from Ruby #Alaska as ...     1.0   \n",
      "\n",
      "   keyword_None  keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
      "0          True           False             False               False   \n",
      "1          True           False             False               False   \n",
      "2          True           False             False               False   \n",
      "3          True           False             False               False   \n",
      "4          True           False             False               False   \n",
      "\n",
      "   keyword_airplane%20accident  keyword_ambulance  ...  keyword_weapons  \\\n",
      "0                        False              False  ...            False   \n",
      "1                        False              False  ...            False   \n",
      "2                        False              False  ...            False   \n",
      "3                        False              False  ...            False   \n",
      "4                        False              False  ...            False   \n",
      "\n",
      "   keyword_whirlwind  keyword_wild%20fires  keyword_wildfire  \\\n",
      "0              False                 False             False   \n",
      "1              False                 False             False   \n",
      "2              False                 False             False   \n",
      "3              False                 False             False   \n",
      "4              False                 False             False   \n",
      "\n",
      "   keyword_windstorm  keyword_wounded  keyword_wounds  keyword_wreck  \\\n",
      "0              False            False           False          False   \n",
      "1              False            False           False          False   \n",
      "2              False            False           False          False   \n",
      "3              False            False           False          False   \n",
      "4              False            False           False          False   \n",
      "\n",
      "   keyword_wreckage  keyword_wrecked  \n",
      "0             False            False  \n",
      "1             False            False  \n",
      "2             False            False  \n",
      "3             False            False  \n",
      "4             False            False  \n",
      "\n",
      "[5 rows x 226 columns] \n",
      "\n",
      "    id location                                               text  target  \\\n",
      "0   0     None                 Just happened a terrible car crash     NaN   \n",
      "1   2     None  Heard about #earthquake is different cities, s...     NaN   \n",
      "2   3     None  there is a forest fire at spot pond, geese are...     NaN   \n",
      "3   9     None           Apocalypse lighting. #Spokane #wildfires     NaN   \n",
      "4  11     None      Typhoon Soudelor kills 28 in China and Taiwan     NaN   \n",
      "\n",
      "   keyword_None  keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
      "0          True           False             False               False   \n",
      "1          True           False             False               False   \n",
      "2          True           False             False               False   \n",
      "3          True           False             False               False   \n",
      "4          True           False             False               False   \n",
      "\n",
      "   keyword_airplane%20accident  keyword_ambulance  ...  keyword_weapons  \\\n",
      "0                        False              False  ...            False   \n",
      "1                        False              False  ...            False   \n",
      "2                        False              False  ...            False   \n",
      "3                        False              False  ...            False   \n",
      "4                        False              False  ...            False   \n",
      "\n",
      "   keyword_whirlwind  keyword_wild%20fires  keyword_wildfire  \\\n",
      "0              False                 False             False   \n",
      "1              False                 False             False   \n",
      "2              False                 False             False   \n",
      "3              False                 False             False   \n",
      "4              False                 False             False   \n",
      "\n",
      "   keyword_windstorm  keyword_wounded  keyword_wounds  keyword_wreck  \\\n",
      "0              False            False           False          False   \n",
      "1              False            False           False          False   \n",
      "2              False            False           False          False   \n",
      "3              False            False           False          False   \n",
      "4              False            False           False          False   \n",
      "\n",
      "   keyword_wreckage  keyword_wrecked  \n",
      "0             False            False  \n",
      "1             False            False  \n",
      "2             False            False  \n",
      "3             False            False  \n",
      "4             False            False  \n",
      "\n",
      "[5 rows x 226 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split back into train and test data\n",
    "train_df = combined_df[:len(train_df)]\n",
    "test_df = combined_df[len(train_df):]\n",
    "print(train_df.head(),'\\n\\n', test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id location                                               text  target  \\\n",
      "0   1     None  Our Deeds are the Reason of this #earthquake M...     1.0   \n",
      "1   4     None             Forest fire near La Ronge Sask. Canada     1.0   \n",
      "2   5     None  All residents asked to 'shelter in place' are ...     1.0   \n",
      "3   6     None  13,000 people receive #wildfires evacuation or...     1.0   \n",
      "4   7  Ruby AK  Just got sent this photo from Ruby #Alaska as ...     1.0   \n",
      "\n",
      "   keyword_None  keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
      "0          True           False             False               False   \n",
      "1          True           False             False               False   \n",
      "2          True           False             False               False   \n",
      "3          True           False             False               False   \n",
      "4          True           False             False               False   \n",
      "\n",
      "   keyword_airplane%20accident  keyword_ambulance  ...  keyword_whirlwind  \\\n",
      "0                        False              False  ...              False   \n",
      "1                        False              False  ...              False   \n",
      "2                        False              False  ...              False   \n",
      "3                        False              False  ...              False   \n",
      "4                        False              False  ...              False   \n",
      "\n",
      "   keyword_wild%20fires  keyword_wildfire  keyword_windstorm  keyword_wounded  \\\n",
      "0                 False             False              False            False   \n",
      "1                 False             False              False            False   \n",
      "2                 False             False              False            False   \n",
      "3                 False             False              False            False   \n",
      "4                 False             False              False            False   \n",
      "\n",
      "   keyword_wounds  keyword_wreck  keyword_wreckage  keyword_wrecked  \\\n",
      "0           False          False             False            False   \n",
      "1           False          False             False            False   \n",
      "2           False          False             False            False   \n",
      "3           False          False             False            False   \n",
      "4           False          False             False            False   \n",
      "\n",
      "                                               input  \n",
      "0  Our Deeds are the Reason of this #earthquake M...  \n",
      "1             Forest fire near La Ronge Sask. Canada  \n",
      "2  All residents asked to 'shelter in place' are ...  \n",
      "3  13,000 people receive #wildfires evacuation or...  \n",
      "4  Just got sent this photo from Ruby #Alaska as ...  \n",
      "\n",
      "[5 rows x 227 columns] \n",
      "\n",
      "    id location                                               text  target  \\\n",
      "0   0     None                 Just happened a terrible car crash     NaN   \n",
      "1   2     None  Heard about #earthquake is different cities, s...     NaN   \n",
      "2   3     None  there is a forest fire at spot pond, geese are...     NaN   \n",
      "3   9     None           Apocalypse lighting. #Spokane #wildfires     NaN   \n",
      "4  11     None      Typhoon Soudelor kills 28 in China and Taiwan     NaN   \n",
      "\n",
      "   keyword_None  keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
      "0          True           False             False               False   \n",
      "1          True           False             False               False   \n",
      "2          True           False             False               False   \n",
      "3          True           False             False               False   \n",
      "4          True           False             False               False   \n",
      "\n",
      "   keyword_airplane%20accident  keyword_ambulance  ...  keyword_whirlwind  \\\n",
      "0                        False              False  ...              False   \n",
      "1                        False              False  ...              False   \n",
      "2                        False              False  ...              False   \n",
      "3                        False              False  ...              False   \n",
      "4                        False              False  ...              False   \n",
      "\n",
      "   keyword_wild%20fires  keyword_wildfire  keyword_windstorm  keyword_wounded  \\\n",
      "0                 False             False              False            False   \n",
      "1                 False             False              False            False   \n",
      "2                 False             False              False            False   \n",
      "3                 False             False              False            False   \n",
      "4                 False             False              False            False   \n",
      "\n",
      "   keyword_wounds  keyword_wreck  keyword_wreckage  keyword_wrecked  \\\n",
      "0           False          False             False            False   \n",
      "1           False          False             False            False   \n",
      "2           False          False             False            False   \n",
      "3           False          False             False            False   \n",
      "4           False          False             False            False   \n",
      "\n",
      "                                               input  \n",
      "0                 Just happened a terrible car crash  \n",
      "1  Heard about #earthquake is different cities, s...  \n",
      "2  there is a forest fire at spot pond, geese are...  \n",
      "3           Apocalypse lighting. #Spokane #wildfires  \n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "\n",
      "[5 rows x 227 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thaddeus Maximus\\AppData\\Local\\Temp\\ipykernel_30768\\3857725947.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.loc[:, 'input'] = train_df.apply(lambda row: f\"{row['text']}\", axis=1)\n",
      "C:\\Users\\Thaddeus Maximus\\AppData\\Local\\Temp\\ipykernel_30768\\3857725947.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.loc[:, 'input'] = test_df.apply(lambda row: f\"{row['text']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "# train_df = train_df.copy()\n",
    "# test_df = test_df.copy()\n",
    "train_df.loc[:, 'input'] = train_df.apply(lambda row: f\"{row['text']}\", axis=1)\n",
    "test_df.loc[:, 'input'] = test_df.apply(lambda row: f\"{row['text']}\", axis=1)\n",
    "\n",
    "print(train_df.head(),'\\n\\n', test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', 'Forest fire near La Ronge Sask. Canada', \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\", '13,000 people receive #wildfires evacuation orders in California ', 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '] \n",
      "\n",
      " [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Extract 'input' as input and 'target' as labels from the train data\n",
    "train_texts = train_df['input'].tolist()\n",
    "train_labels = train_df['target'].tolist()\n",
    "print(train_texts[:5],'\\n\\n', train_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just happened a terrible car crash', 'Heard about #earthquake is different cities, stay safe everyone.', 'there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all', 'Apocalypse lighting. #Spokane #wildfires', 'Typhoon Soudelor kills 28 in China and Taiwan'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the test data, we only need the 'input' column\n",
    "test_texts = test_df['input'].tolist()\n",
    "print(test_texts[:5],'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 6090\n",
      "Validation set size: 1523\n"
     ]
    }
   ],
   "source": [
    "# Split the train data into train and validation subsets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "# Print the sizes of the training and validation subsets\n",
    "print(f\"Training set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datasets\n",
    "train_dataset = TweetDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TweetDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = TweetDataset(test_texts, None, tokenizer)  # No labels for the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # num_labels=2 for binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m \u001b[39m# Define the training arguments\u001b[39;00m\n\u001b[0;32m      4\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../models/results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,              \u001b[39m# total number of training epochs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m,     \u001b[39m# evaluation is performed at the end of each epoch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1088\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1089\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1090\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1091\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1098\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:39\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[0;32m     37\u001b[0m \u001b[39m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mintegrations\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     41\u001b[0m     hp_params,\n\u001b[0;32m     42\u001b[0m     is_fairscale_available,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[39m# isort: on\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\integrations.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# comet_ml requires to be imported before any ML frameworks\u001b[39;00m\n\u001b[0;32m     43\u001b[0m _has_comet \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mfind_spec(\u001b[39m\"\u001b[39m\u001b[39mcomet_ml\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mCOMET_MODE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mupper() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDISABLED\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:1465\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m library\n\u001b[0;32m   1464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   1467\u001b[0m \u001b[39m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mTORCH_CUDA_SANITIZER\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_meta_registrations.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m \u001b[39mimport\u001b[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mimport\u001b[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_decomp\\__init__.py:169\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m decompositions\n\u001b[0;32m    168\u001b[0m \u001b[39m# populate the table\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecompositions\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_refs\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# This list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mprims\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_prims\\__init__.py:33\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     check,\n\u001b[0;32m     19\u001b[0m     Dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     type_to_dtype,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m backwards_not_supported\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeTensor, FakeTensorMode\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytree\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_subclasses\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     DynamicOutputShapeException,\n\u001b[0;32m      5\u001b[0m     FakeTensor,\n\u001b[0;32m      6\u001b[0m     FakeTensorMode,\n\u001b[0;32m      7\u001b[0m     UnsupportedFakeTensorException,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossRefFakeMode\n\u001b[0;32m     12\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensorMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCrossRefFakeMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mweakref\u001b[39;00m \u001b[39mimport\u001b[39;00m ReferenceType\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_guards\u001b[39;00m \u001b[39mimport\u001b[39;00m Source\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     elementwise_dtypes,\n\u001b[0;32m     17\u001b[0m     ELEMENTWISE_TYPE_PROMOTION_KIND,\n\u001b[0;32m     18\u001b[0m     is_float_dtype,\n\u001b[0;32m     19\u001b[0m     is_integer_dtype,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_guards.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# TODO(voz): Stolen pattern, not sure why this is the case,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# but mypy complains.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msympy\u001b[39;00m  \u001b[39m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mNo sympy found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\__init__.py:74\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m \u001b[39mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[0;32m     68\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[0;32m     69\u001b[0m         true, false, satisfiable)\n\u001b[0;32m     71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massumptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[0;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m \u001b[39mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[0;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[0;32m     77\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[0;32m     78\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[0;32m     79\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[0;32m     80\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, real_roots,\n\u001b[0;32m     81\u001b[0m         nroots, ground_roots, nth_power_roots_poly, cancel, reduced, groebner,\n\u001b[0;32m     82\u001b[0m         is_zero_dimensional, GroebnerBasis, poly, symmetrize, horner,\n\u001b[0;32m     83\u001b[0m         interpolate, rational_interpolate, viete, together,\n\u001b[0;32m     84\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[0;32m     85\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[0;32m     86\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[0;32m     87\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[0;32m     88\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[0;32m     89\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[0;32m     90\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[0;32m     91\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[0;32m     92\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[0;32m     93\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[0;32m     94\u001b[0m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[0;32m     95\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[0;32m     96\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[0;32m     97\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[0;32m     98\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[0;32m     99\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[0;32m    100\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[0;32m    101\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[0;32m    102\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[0;32m    103\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[0;32m    104\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[0;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[0;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[0;32m    108\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mseries\u001b[39;00m \u001b[39mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[0;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[0;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[0;32m    112\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[0;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[0;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[0;32m    134\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\__init__.py:68\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Polynomial manipulation algorithms and algebraic objects. \"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPoly\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPurePoly\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpoly_from_expr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparallel_poly_from_expr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdegree\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtotal_degree\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdegree_list\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpdiv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpquo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfield\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mxfield\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvfield\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msfield\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     66\u001b[0m ]\n\u001b[1;32m---> 68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolytools\u001b[39;00m \u001b[39mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr,\n\u001b[0;32m     69\u001b[0m         parallel_poly_from_expr, degree, total_degree, degree_list, LC, LM,\n\u001b[0;32m     70\u001b[0m         LT, pdiv, prem, pquo, pexquo, div, rem, quo, exquo, half_gcdex, gcdex,\n\u001b[0;32m     71\u001b[0m         invert, subresultants, resultant, discriminant, cofactors, gcd_list,\n\u001b[0;32m     72\u001b[0m         gcd, lcm_list, lcm, terms_gcd, trunc, monic, content, primitive,\n\u001b[0;32m     73\u001b[0m         compose, decompose, sturm, gff_list, gff, sqf_norm, sqf_part,\n\u001b[0;32m     74\u001b[0m         sqf_list, sqf, factor_list, factor, intervals, refine_root,\n\u001b[0;32m     75\u001b[0m         count_roots, real_roots, nroots, ground_roots, nth_power_roots_poly,\n\u001b[0;32m     76\u001b[0m         cancel, reduced, groebner, is_zero_dimensional, GroebnerBasis, poly)\n\u001b[0;32m     78\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolyfuncs\u001b[39;00m \u001b[39mimport\u001b[39;00m (symmetrize, horner, interpolate,\n\u001b[0;32m     79\u001b[0m         rational_interpolate, viete)\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrationaltools\u001b[39;00m \u001b[39mimport\u001b[39;00m together\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\polytools.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraversal\u001b[39;00m \u001b[39mimport\u001b[39;00m preorder_traversal, bottom_up\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mboolalg\u001b[39;00m \u001b[39mimport\u001b[39;00m BooleanAtom\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m \u001b[39mimport\u001b[39;00m polyoptions \u001b[39mas\u001b[39;00m options\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstructor\u001b[39;00m \u001b[39mimport\u001b[39;00m construct_domain\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdomains\u001b[39;00m \u001b[39mimport\u001b[39;00m FF, QQ, ZZ\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    evaluation_strategy='epoch',     # evaluation is performed at the end of each epoch\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # function to compute metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='../models/results',  # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=16,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='../models/logs',    # directory for storing logs\n",
    "#     learning_rate=1e-5,              # learning rate\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                     # the instantiated  Transformers model to be trained\n",
    "#     args=training_args,              # training arguments, defined above\n",
    "#     train_dataset=train_dataset,     # training dataset\n",
    "#     eval_dataset=val_dataset,        # evaluation dataset\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir='../models/results',\n",
    "#     num_train_epochs=3,  # reduce from 5 to 3\n",
    "#     per_device_train_batch_size=32,  # increase from 16 to 32\n",
    "#     gradient_accumulation_steps=2,  # new line for gradient accumulation\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     learning_rate=3e-5,  # increase from 2e-5 to 3e-5\n",
    "#     logging_dir='../models/logs',\n",
    "#     logging_steps=10,  # Log every 10 steps\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluation and Save happens at every epoch\n",
    "#     save_strategy=\"epoch\",  # Save the model after every epoch\n",
    "#     load_best_model_at_end=True,\n",
    "#     #fp16=True,  # new line for mixed precision training\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer with the correct datasets\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,  # Use validation dataset for evaluation\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "conf_matrix = confusion_matrix(train_texts, train_preds, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n",
    "                              display_labels=[0, 1])\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Output to CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on test data\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# We take the output class with the highest probability\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Prepare a DataFrame with test IDs and predictions\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],  # Assuming that 'test_df' is the DataFrame with your test data\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "# Save the DataFrame into a CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above script, `compute_metrics` is a function that calculates precision, recall, f1, and accuracy. This function is then passed to the `Trainer`, which uses it to compute these metrics after each evaluation.\n",
    "\n",
    "`TrainingArguments` is initialized with parameters that dictate when logging and saving should occur. `logging_steps=10` means that the training loss is logged every 10 steps, and `evaluation_strategy=\"epoch\"` and `save_strategy=\"epoch\"` mean that an evaluation and a model save are performed after every epoch, respectively.\n",
    "\n",
    "By setting `load_best_model_at_end=True`, the `Trainer` will automatically load the best model in terms of evaluation loss when the training is finished.\n",
    "\n",
    "This assumes you want to perform evaluation and save the model after every epoch and log every 10 steps, but you might want to adjust these values depending on the size of your dataset and the resources you have available.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hyperparameters in a machine learning model are parameters whose values are set before the learning process begins and they play a crucial role in the performance of the model. \n",
    "\n",
    "In the context of the Hugging Face's `Trainer`, here are some important hyperparameters and how to set them:\n",
    "\n",
    "1. **Learning Rate**: This is one of the most important hyperparameters. If it's too high, training may diverge; if it's too low, training may be too slow or get stuck in a poor local minimum. You can set it in `TrainingArguments` with the `learning_rate` argument.\n",
    "\n",
    "2. **Batch Size**: This is the number of samples that will be propagated through the network at once. A larger batch size allows you to leverage hardware optimizations and train your model faster, but it might also result in worse generalization performance. You can set it in `TrainingArguments` with the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments.\n",
    "\n",
    "3. **Number of Epochs**: This is the number of times the entire dataset is passed forward and backward through the network. You can set it in `TrainingArguments` with the `num_train_epochs` argument.\n",
    "\n",
    "4. **Weight Decay**: This is a regularization technique that helps prevent your model from overfitting. You can set it in `TrainingArguments` with the `weight_decay` argument.\n",
    "\n",
    "Here's an example of setting these hyperparameters:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,  # from 3 to 5\n",
    "    per_device_train_batch_size=32,  # from 16 to 32\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,  # added this line to set learning rate\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "```\n",
    "\n",
    "In this example, the number of epochs is increased to 5, the training batch size is increased to 32, and the learning rate is set to 2e-5. \n",
    "\n",
    "Hyperparameter tuning can be a complex process and usually involves trial and error. It might be beneficial to perform a systematic hyperparameter search or optimization using tools like Optuna or Ray Tune, especially for larger and more complex models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
